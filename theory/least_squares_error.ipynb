{
 "metadata": {
  "name": "",
  "signature": "sha256:f77dd3d3f1126236c4f17659da5ff1220f3183699762c3657124587a9885d86f"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Notebook description"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This notebook examines the error that results from the [least squares](http://nbviewer.ipython.org/github/fragapanagos/notebooks/blob/master/theory/least_squares.ipynb).\n",
      "\n",
      "In least squares, our goal is to approximate a desired vector, $\\mathbf{y}$, as the linear combination of vectors $\\mathbf{a}_i$, using weights $\\mathbf{x}$. That is, we would like\n",
      "\n",
      "$$\n",
      "\\mathbf{y}\\approx\\mathbf{A}\\mathbf{x}\n",
      "$$\n",
      "\n",
      "We will see that under some conditions $\\mathbf{A}\\mathbf{x}=\\mathbf{y}$ exactly, but under other conditions we can only approximate $\\mathbf{y}$. To find the best approximation of $\\mathbf{y}$, the least squares technique formulates the problem as\n",
      "\n",
      "$$\n",
      "\\min\\|\\mathbf{y}-\\mathbf{A}\\mathbf{x}\\|_2\n",
      "$$\n",
      "\n",
      "or rather finding $\\hat{\\mathbf{x}}$ where\n",
      "\n",
      "$\\DeclareMathOperator*{\\argmin}{arg\\,min}$\n",
      "$$\n",
      "\\hat{\\mathbf{x}}=\\argmin_{\\mathbf{x}}\\|\\mathbf{y}-\\mathbf{A}\\mathbf{x}\\|_2\n",
      "$$\n",
      "\n",
      "\n",
      "Specifically, we would like to know: \n",
      " - How accurate is our approximation of $\\mathbf{y}$? That is, what is $\\|\\mathbf{y}-\\mathbf{A}\\hat{\\mathbf{x}}\\|_2$?"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Summary of results"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the case where there is no noise in $\\mathbf{A}$ or $\\mathbf{y}$,\n",
      "\n",
      "\\begin{align}\n",
      "\\|\\mathbf{y}-\\mathbf{A}\\hat{\\mathbf{x}}\\|_2 &= \\left\\|\\mathbf{U}_{\\sigma=0}^T\\mathbf{y}\\right\\|_2 \\\\\n",
      " &= \\left\\|\\sum_{i=r+1}^m\\mathbf{u}_i\\mathbf{u}_i^T\\mathbf{y}\\right\\|_2 \\\\\n",
      "\\end{align}\n",
      "\n",
      "In the noisy case ... TODO"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Notation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The desired function is described with $m$ sample points and could be a single-dimensional or multi-dimensional function. In the single dimensional case, $\\mathbf{y}\\in\\mathbb{R}^{m}$, $\\mathbf{A}\\in\\mathbb{R}^{m\\times n}$,  $\\mathbf{x}\\in\\mathbb{R}^{n}$, and\n",
      "\n",
      "$$\n",
      "\\begin{bmatrix}\n",
      "y_0 \\\\\n",
      "y_1 \\\\\n",
      "\\vdots \\\\\n",
      "y_m \\\\\n",
      "\\end{bmatrix}\n",
      "\\approx\n",
      "\\begin{bmatrix}\n",
      "A_{0,0} & A_{0,1} & \\cdots & A_{0,n} \\\\\n",
      "A_{1,0} & A_{1,1} & \\cdots & A_{1,n} \\\\\n",
      "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
      "A_{m,0} & A_{m,1} & \\cdots & A_{m,n} \\\\\n",
      "\\end{bmatrix}\n",
      "\\begin{bmatrix}\n",
      "x_0 \\\\\n",
      "x_1 \\\\\n",
      "\\vdots \\\\\n",
      "x_n \\\\\n",
      "\\end{bmatrix}\n",
      "$$\n",
      "\n",
      "Or put another way,\n",
      "\n",
      "$$\n",
      "\\begin{bmatrix}\n",
      "| \\\\\n",
      "\\mathbf{y} \\\\\n",
      "| \\\\\n",
      "\\end{bmatrix}\n",
      "\\approx\n",
      "\\begin{bmatrix}\n",
      "| & | &  & | \\\\\n",
      "\\mathbf{a}_0 & \\mathbf{a}_1 & \\cdots & \\mathbf{a}_n \\\\\n",
      "| & | &  & | \n",
      "\\end{bmatrix}\n",
      "\\begin{bmatrix}\n",
      "| \\\\\n",
      "\\mathbf{x} \\\\\n",
      "| \\\\\n",
      "\\end{bmatrix}\n",
      "$$\n",
      "\n",
      "In the multidimensional case, $\\mathbf{y}\\in\\mathbb{R}^{m\\times d}$, $\\mathbf{A}\\in\\mathbb{R}^{m\\times n}$, and $\\mathbf{x}\\in\\mathbb{R}^{n\\times d}$. However the multidimensional case is [separable](#Separability-of-decoding-dimensions), so we need only analyze the single dimensional case."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Background"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Our analysis centers on the properties of $\\mathbf{A}$ in relation to the operation $\\mathbf{A}\\mathbf{x}$, which is a linear map between two vector spaces $\\mathbb{R}^n$ and $\\mathbb{R}^m$ where $\\mathbf{x}\\in\\mathbb{R}^n$ and $\\mathbf{y}\\in\\mathbb{R}^m$. This linear map has four fundamental subspaces:\n",
      " - The set of vectors $\\mathbf{y}\\in\\mathbb{R}^m$ such that $\\mathbf{y}=\\mathbf{A}\\mathbf{x}$ for any $\\mathbf{x}\\in\\mathbb{R}^n$ is the _column space_ of $\\mathbf{A}$. The dimensionality of the column space is $r$, the _rank_ of $\\mathbf{A}$.\n",
      " - The set of vectors $\\mathbf{x}\\in\\mathbb{R}^n$ such that $\\mathbf{A}^T\\mathbf{y}=\\mathbf{x}$ for any $\\mathbf{y}\\in\\mathbb{R}^m$ is the _row space_ of $\\mathbf{A}$. The dimensionality of the row space is also $r$.\n",
      " - The set of vectors $\\mathbf{x}\\in\\mathbb{R}^n$ such that $\\mathbf{0}=\\mathbf{A}\\mathbf{x}$ is the _nullspace_. The dimensionality of the nullspace is $(n-r)$, the _nullity_ of $\\mathbf{A}$. The nullspace is the complement of the row space.\n",
      " - The set of vectors $\\mathbf{y}\\in\\mathbb{R}^m$ such that $\\mathbf{A}^T\\mathbf{y}=0$ is the _left nullspace_. The dimensionality of the left nullspace is $(m-r)$, the corank of $\\mathbf{A}$. The left null space is the complement of the column space.\n",
      "\n",
      "More specifically, our analysis will focus on the left null space of $\\mathbf{A}$. The column space and the left nullspace are complementary subsets of $\\mathbb{R}^m$, so they span $\\mathbb{R}^m$ and divide it into two subspaces. The column space is the space of all vectors that we could possibly reach for some $\\mathbf{x}$ with $\\mathbf{A}\\mathbf{x}$. The left nullspace is the space of all vectors that are impossible for us to reach using $\\mathbf{A}\\mathbf{x}$ for any $\\mathbf{x}$. At this point, we can begin to intuit where error from our linear approximation of $\\mathbf{y}$ will come from.  If there $\\mathbf{y}$ has components in the left null space of $\\mathbf{A}$, then it will be impossible for us to recreate $\\mathbf{y}$ exactly for any $\\mathbf{x}$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To express $\\mathbf{y}$ in terms of the column space and the left null space, we will use the SVD. $\\mathbf{A}$ can be decomposed via SVD as\n",
      "\n",
      "\\begin{align}\n",
      "\\mathbf{A} &= \\mathbf{U}\\Sigma\\mathbf{V}^T \\\\\n",
      " &= \\sum_{i=1}^{r}\\sigma_i\\mathbf{u}_i\\mathbf{v}_i^T \\\\\n",
      "\\end{align}\n",
      "\n",
      "where\n",
      " - $\\mathbf{U}\\in\\mathbb{R}^{m\\times m}$\n",
      " - $\\mathbf{V}\\in\\mathbb{R}^{n\\times n}$\n",
      " - $\\Sigma\\in\\mathbb{R}^{m\\times n}$\n",
      " \n",
      "and\n",
      " - the columns of $\\mathbf{U}$ (the _left singular vectors_) form an orthogonal basis for $\\mathbb{R}^m$,\n",
      " - the columns of $\\mathbf{V}$ (the _right singular vectors_) form an orthogonal basis for $\\mathbb{R}^n$,\n",
      " - $\\Sigma$ is diagonal rectangular with diagonal entries $\\sigma_i$, and the number of nonzero $\\sigma_i$ is $r$. \n",
      "\n",
      "Therefore\n",
      " - the right singular vectors corresponding to the zero entries of $\\Sigma$ form an orthogonal basis for the nullspace of $\\mathbf{A}$,\n",
      " - the right singular vectors corresponding to the nonzero entries of $\\Sigma$ form an orthogonal basis for the row space of $\\mathbf{A}$,\n",
      " - the left singular vectors corresponding to the zero entries of $\\Sigma$ form an orthogonal basis for the left nullspace of $\\mathbf{A}$.\n",
      " - the left singular vectors corresponding to the nonzero entries of $\\Sigma$ form an orthogonal basis for the column space of $\\mathbf{A}$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The least squares-optimal $\\mathbf{x}$, $\\hat{\\mathbf{x}}$, that minimizes $\\|\\mathbf{y}-\\mathbf{A}\\mathbf{x}\\|_2$ is given by the pseudoinverse\n",
      "\n",
      "$$\n",
      "\\hat{\\mathbf{x}}=\\mathbf{A}^+\\mathbf{y}\n",
      "$$\n",
      "\n",
      "where \n",
      "\n",
      "$$\n",
      "\\mathbf{A}^+=\\mathbf{V}\\Sigma^+\\mathbf{U}^T\n",
      "$$\n",
      "\n",
      "and $\\Sigma^+$ is computed by setting the nonzero entries of $\\Sigma$ to their reciprocal, leaving the $0$s in place, and transposing the matrix, so $\\Sigma^+\\in\\mathbb{R}^{n\\times m}$. Also,\n",
      " - In the case that the columns of $\\mathbf{A}$ are linearly independent, \n",
      " $\\mathbf{A}^+=(\\mathbf{A}^T\\mathbf{A})^{-1}\\mathbf{A}^T$\n",
      " - In the case that the rows of $\\mathbf{A}$ are linearly independent,\n",
      " $\\mathbf{A}^+=\\mathbf{A}^T(\\mathbf{A}\\mathbf{A}^T)^{-1}$\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Analysis"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Using the SVD, we can determine exactly how much error we can expect when approximating $\\mathbf{y}$ using $\\mathbf{A}\\mathbf{x}$. The error in our approximation will be exactly captured in the left null space of $\\mathbf{A}$. That is, we cannot possibly reach anywhere within the left nullspace using $\\mathbf{A}\\mathbf{x}$ for any $\\mathbf{x}\\in\\mathbb{R}^n$. Therefore, we should consider the projection of $\\mathbf{y}$ onto the columns of $\\mathbf{U}$ that form a basis for the left null space of $\\mathbf{A}$."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Column space and the left null space projections"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This projection is expressed as \n",
      "\\begin{align}\n",
      "\\mathbf{y} &= \\mathbf{U}\\mathbf{U}^T\\mathbf{y} \\\\\n",
      " &= \\sum_{i=1}^mu_iu_i^Ty \\\\\n",
      " &= \\sum_{i=1}^ru_iu_i^Ty + \\sum_{j=r+1}^mu_ju_j^Ty \\\\\n",
      "\\end{align}\n",
      "\n",
      "where we've separated $u_i$ into vectors in the column space of $\\mathbf{A}$ and vectors in the left null space of $\\mathbf{A}$. Another way of expressing this is\n",
      "\n",
      "$$\n",
      "\\mathbf{y} = \\mathbf{U}(\\mathbf{U}_{\\sigma\\neq0}^T\\mathbf{y}+\\mathbf{U}_{\\sigma=0}^T\\mathbf{y})\n",
      "$$\n",
      "\n",
      "where \n",
      "\n",
      "$$\n",
      "\\mathbf{U}_{\\sigma\\neq0} =\n",
      "\\begin{bmatrix}\n",
      "| & & | & | & & | \\\\\n",
      "u_1 & \\cdots & u_r & 0 & \\cdots & 0 \\\\ \n",
      "| & & | & | & & | \\\\\n",
      "\\end{bmatrix}\n",
      "$$\n",
      "\n",
      "and \n",
      "\n",
      "$$\n",
      "\\mathbf{U}_{\\sigma=0} =\n",
      "\\begin{bmatrix}\n",
      "| & & | & | & & | \\\\\n",
      "0 & \\cdots & 0 & u_{r+1} & \\cdots & u_m \\\\ \n",
      "| & & | & | & & | \\\\\n",
      "\\end{bmatrix}\n",
      "$$\n",
      "\n",
      "That is,\n",
      " - $\\mathbf{U}_{\\sigma\\neq0}$ contains the left singular vectors corresponding to the nonzero entries of $\\Sigma$, and \n",
      " - $\\mathbf{U}_{\\sigma=0}$ contains the left singular vectors corresponding to the zero entries of $\\Sigma$.\n",
      "\n",
      "\n",
      "Projecting $\\mathbf{y}$ onto $\\mathbf{U}_{\\sigma\\neq0}$ and $\\mathbf{U}_{\\sigma=0}$ expresses $\\mathbf{y}$ as the combination of a column space projection and a left null space projection."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Analysis of the pseudoinverse solution"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Setting aside our column space and left null space projection of $\\mathbf{y}$ for the moment, we now analyze the least squares solution obtained using the pseudoinverse:\n",
      "\n",
      "$$\n",
      "\\hat{\\mathbf{x}} = \\mathbf{A}^+\\mathbf{y}\n",
      "$$\n",
      "\n",
      "so \n",
      "\n",
      "\\begin{align}\n",
      "\\mathbf{A}\\hat{\\mathbf{x}} &= \\mathbf{A}\\mathbf{A}^+\\mathbf{y} \\\\\n",
      " &= \\mathbf{U}\\Sigma\\mathbf{V}^T\\mathbf{V}\\Sigma^+\\mathbf{U}^T\\mathbf{y} \\\\\n",
      " &= \\mathbf{U}\\Sigma\\Sigma^+\\mathbf{U}^T\\mathbf{y} \\\\\n",
      "\\end{align}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "At this point, we need to be careful. What becomes of $\\Sigma\\Sigma^+$ depends on the rank and dimensions of $\\mathbf{A}$. Recall that $\\Sigma\\in\\mathbb{R}^{m\\times n}$ and $\\Sigma^+\\in\\mathbb{R}^{n\\times m}$ are both diagonal rectangular with $r$ nonzero entries, so $\\Sigma\\Sigma^+\\in\\mathbb{R}^{m\\times m}$ and has $r$-$1$s along the diagonal. $r\\le m$ always so we have two cases to consider: $r=m$ and $r<m$. \n",
      "\n",
      "When $r=m$, $\\Sigma\\Sigma^+=\\mathbf{I}$, so\n",
      "\n",
      "\\begin{align}\n",
      "\\mathbf{A}\\hat{\\mathbf{x}} &= \\mathbf{U}\\Sigma\\Sigma^+\\mathbf{U}^T\\mathbf{y} \\\\\n",
      " &= \\mathbf{U}\\mathbf{I}\\mathbf{U}^T\\mathbf{y} \\\\\n",
      " &= \\mathbf{U}\\mathbf{U}^T\\mathbf{y} \\\\\n",
      " &= \\mathbf{y} \\\\\n",
      "\\end{align}\n",
      "\n",
      "At this point, we see that when $r=m$, we can exactly reconstruct $\\mathbf{y}$ using $\\mathbf{A}\\hat{\\mathbf{x}}$ for any $\\mathbf{y}$, so our approximation error will be \n",
      "\n",
      "\\begin{align}\n",
      "\\|\\mathbf{y}-\\mathbf{A}\\hat{\\mathbf{x}}\\|_2 &= \\|\\mathbf{y}-\\mathbf{y}\\|_2 \\\\\n",
      "&= 0 \\\\\n",
      "\\end{align}\n",
      "\n",
      "When $r<m$, $\\Sigma\\Sigma^+=\\mathbf{I}_r$ where $\\mathbf{I}_r$ has only $r$ ones along the diagonal, so\n",
      "\n",
      "\\begin{align}\n",
      "\\mathbf{A}\\hat{\\mathbf{x}} &= \\mathbf{U}\\Sigma\\Sigma^+\\mathbf{U}^T\\mathbf{y} \\\\\n",
      " &= \\mathbf{U}\\mathbf{I}_r\\mathbf{U}^T\\mathbf{y} \\\\\n",
      " &= \\sum_{i=1}^m\\mathbf{i}^r_{i}\\mathbf{u}_i\\mathbf{u}_i^T\\mathbf{y} \\\\\n",
      " &= \\sum_{i=1}^r\\mathbf{i}^r_{i}\\mathbf{u}_i\\mathbf{u}_i^T\\mathbf{y}+\n",
      "     \\sum_{i=r+1}^m\\mathbf{i}^r_{i}\\mathbf{u}_i\\mathbf{u}_i^T\\mathbf{y} \\\\\n",
      " &= \\sum_{i=1}^r\\mathbf{u}_i\\mathbf{u}_i^T\\mathbf{y} \\\\\n",
      "\\end{align}\n",
      "\n",
      "Another way of saying this is\n",
      "\n",
      "\\begin{align}\n",
      "\\mathbf{A}\\hat{\\mathbf{x}} &= \\mathbf{U}\\Sigma\\Sigma^+\\mathbf{U}^T\\mathbf{y} \\\\\n",
      " &= \\mathbf{U}\\mathbf{I}_r\\mathbf{U}^T\\mathbf{y} \\\\\n",
      " &= \\mathbf{U}\\mathbf{U}_{\\sigma\\neq0}^T\\mathbf{y} \\\\\n",
      "\\end{align}\n",
      "\n",
      "At this point, [recall](#Column-space-and-the-left-null-space-projections) our projections of $\\mathbf{y}$ onto the column space and the left null space of $\\mathbf{A}$. We see that when $r<m$, we can only reconstruct the components of $\\mathbf{y}$ that fall within the column space of $\\mathbf{A}$, so our approximation error will be\n",
      "\n",
      "\\begin{align}\n",
      "\\|\\mathbf{y}-\\mathbf{A}\\hat{\\mathbf{x}}\\|_2 &= \n",
      "    \\left\\|\\mathbf{U}(\\mathbf{U}_{\\sigma\\neq0}^T\\mathbf{y}+\\mathbf{U}_{\\sigma=0}^T\\mathbf{y})\n",
      "    -\\mathbf{U}\\mathbf{U}_{\\sigma\\neq0}^T\\mathbf{y}\\right\\|_2 \\\\\n",
      " &= \\left\\|\\mathbf{U}\\mathbf{U}_{\\sigma=0}^T\\mathbf{y}\\right\\|_2 \\\\\n",
      " &= \\left\\|\\mathbf{U}_{\\sigma=0}^T\\mathbf{y}\\right\\|_2 & \\mathbf{U} \\text{ is a rotation matrix}\\\\\n",
      "\\end{align}\n",
      "\n",
      "which is the norm of $\\mathbf{y}$ projected onto the left null space of $\\mathbf{A}$. Put in summation notation,\n",
      "\n",
      "$$\n",
      "\\|\\mathbf{y}-\\mathbf{A}\\hat{\\mathbf{x}}\\|_2 = \n",
      "    \\left\\|\\sum_{i=r+1}^m\\mathbf{u}_i\\mathbf{u}_i^T\\mathbf{y}\\right\\|_2\n",
      "$$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "NEF applications"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "NEF uses least squares to linear approximate some desired function using the weighted combination of neural tuning curves.\n",
      "\n",
      "| $A_{m\\times n}$ is    | full rank | Not full rank |\n",
      "|-------:|:---------:|:-------------:|\n",
      "|__square__,   $m=n$|    $I_{m\\times m}$       |    $I_{m\\times m,\\ r}$           |\n",
      "|__fat__,   $m<n$|    sdfs       |    sdfs           |\n",
      "|__skinny__,  $m>n$|    sdfs       |    sdfs           |"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Appendix"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Separability of decoding dimensions"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "With no other constraints on $\\mathbf{x}$,\n",
      "$$\n",
      "\\begin{bmatrix}\n",
      "| & | & & | \\\\\n",
      "\\mathbf{y}_0 & \\mathbf{y}_1 & \\cdots & \\mathbf{y}_d \\\\\n",
      "| & | & & | \\\\\n",
      "\\end{bmatrix}\n",
      "=\n",
      "\\begin{bmatrix}\n",
      "| & | &  & | \\\\\n",
      "\\mathbf{a}_0 & \\mathbf{a}_1 & \\cdots & \\mathbf{a}_n \\\\\n",
      "| & | &  & | \n",
      "\\end{bmatrix}\n",
      "\\begin{bmatrix}\n",
      "| & | & & | \\\\\n",
      "\\mathbf{x}_0 & \\mathbf{x}_1 & \\cdots & \\mathbf{x}_d \\\\\n",
      "| & | & & | \\\\\n",
      "\\end{bmatrix}\n",
      "$$\n",
      "\n",
      "can be separated into\n",
      "\\begin{align}\n",
      "\\begin{bmatrix}\n",
      "    | \\\\\n",
      "    \\mathbf{y}_0 \\\\\n",
      "    | \\\\\n",
      "\\end{bmatrix}\n",
      "&=\n",
      "\\begin{bmatrix}\n",
      "    | & | &  & | \\\\\n",
      "    \\mathbf{a}_0 & \\mathbf{a}_1 & \\cdots & \\mathbf{a}_n \\\\\n",
      "    | & | &  & | \n",
      "\\end{bmatrix}\n",
      "\\begin{bmatrix}\n",
      "    | \\\\\n",
      "    \\mathbf{x}_0 \\\\\n",
      "    | \\\\\n",
      "\\end{bmatrix} \\\\ \\\\\n",
      "\\begin{bmatrix}\n",
      "    | \\\\\n",
      "    \\mathbf{y}_1 \\\\\n",
      "    | \\\\\n",
      "\\end{bmatrix}\n",
      "&=\n",
      "\\begin{bmatrix}\n",
      "    | & | &  & | \\\\\n",
      "    \\mathbf{a}_0 & \\mathbf{a}_1 & \\cdots & \\mathbf{a}_n \\\\\n",
      "    | & | &  & | \n",
      "\\end{bmatrix}\n",
      "\\begin{bmatrix}\n",
      "    | \\\\\n",
      "    \\mathbf{x}_1 \\\\\n",
      "    | \\\\\n",
      "\\end{bmatrix} \\\\\n",
      "&\\ \\vdots \\\\\n",
      "\\begin{bmatrix}\n",
      "    | \\\\\n",
      "    \\mathbf{y}_d \\\\\n",
      "    | \\\\\n",
      "\\end{bmatrix}\n",
      "&=\n",
      "\\begin{bmatrix}\n",
      "    | & | &  & | \\\\\n",
      "    \\mathbf{a}_0 & \\mathbf{a}_1 & \\cdots & \\mathbf{a}_n \\\\\n",
      "    | & | &  & | \n",
      "\\end{bmatrix}\n",
      "\\begin{bmatrix}\n",
      "    | \\\\\n",
      "    \\mathbf{x}_d \\\\\n",
      "    | \\\\\n",
      "\\end{bmatrix} \\\\ \\\\\n",
      "\\end{align}\n",
      "\n",
      "where we can consider each equation separately.\n",
      "\n",
      "Note that there are times when we do impose constraints on $\\mathbf{x}$. For example, when using the generalized form of least squares to find $\\mathbf{x}$, we seek\n",
      "\n",
      "$\\DeclareMathOperator*{\\argmin}{arg\\,min}$\n",
      "$$\n",
      "\\argmin_x \\|\\mathbf{A}\\mathbf{x}-\\mathbf{y}\\|_2^2+\\|\\mathbf{\\Gamma}\\mathbf{x}\\|_2^2\n",
      "$$\n",
      "\n",
      "Here, $\\mathbf{\\Gamma}$ could impose a relationship between the columns of $\\mathbf{x}$."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}
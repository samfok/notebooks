{
 "metadata": {
  "name": "",
  "signature": "sha256:365b18e3dc02aac02c5b06d7df8a7e0a060188c6a4ebf991fa6a401bf03fca62"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Notebook description"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This notebook looks at the encoding of a variable as a spike rate. Spike rate encoding is a means of giving a variable meaning to neurons. In this encoding scheme, spikes are the input to neurons and spikes are the output of neurons. We have a few variations of spike rate encoding:\n",
      "\n",
      "1) linear encoding\n",
      "\n",
      "2) affine encoding\n",
      "\n",
      "2) fpfm encoding\n",
      "\n",
      "The choice of encoding scheme affects how we train our decoders and how we connect pools of neurons in our network. \n",
      "\n",
      "**Terminology:**\n",
      "\n",
      "A variable $x$ is represented by a firing rate $f(x)$. A pool of neurons has $N$ neurons is assigned an $N\\times e$ encoding matrix, $\\boldsymbol{e}$, which converts $f(x)$ to a firing rate input the excitatory or inhibitory synapse of each neuron in the pool. These firing rate inputs are denoted $f_e(x)$ and $f_i(x)$, respectively. The activity of the neuron pool in response to $x$ is described by the population activity vector, $\\boldsymbol{a}(x)$, where each element $a_i(x)$ is the firing rate for the $i$th neuron in response to $x$. We find a $d\\times N$ decoding matrix, $\\boldsymbol{d}$, that we can use to linearly decode the encoding of a target function $g(x)$.\n",
      "\n",
      "---"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "linear encoding"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In linear encoding, a $x$ is encoded as $f(x)$ by linearly scaling $x$:\n",
      "\n",
      "$$f(x) = f_{max}x$$\n",
      "so\n",
      "$$x = \\frac{f(x)}{f_{max}}$$\n",
      "where $f_{max}$ is a maximum firing rate used to represent a variable. Implicit in this definition of the maximum firing rate is the assumption that $-1\\le x\\le 1$.  \n",
      "\n",
      "$f_e$ and $f_i$ will then be\n",
      "\n",
      "\\begin{align}\n",
      "f_e(x) &= \\max(\\boldsymbol{e}f(x),0) \\quad \\text{(i.e. the positive entries of }\\boldsymbol{e}f(x))\\\\\n",
      "f_i(x) &= \\max(-\\boldsymbol{e}f(x),0) \\quad \\text{(i.e. the negative entries of }\\boldsymbol{e}f(x)).\n",
      "\\end{align}\n",
      "\n",
      "Note that multiplying $f(x)$ by the encoding matrix, $\\boldsymbol{e}$, is equivalent to encoding $\\boldsymbol{e}x$ as a spike rate:\n",
      "\n",
      "\\begin{align}\n",
      "\\boldsymbol{e}f(x) &= \\boldsymbol{e}f_{max}x \\\\\n",
      " &= f_{max}\\boldsymbol{e}x \\\\\n",
      " &= f(\\boldsymbol{e}x)\n",
      "\\end{align}\n",
      "\n",
      "This is a key property that makes the linear encoding very simple relative to the other rate encoding techniques."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "linear encoding: training"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "When computing $g(x)$ to send to another pool, we find a decode matrix, $\\boldsymbol{d}$, such that\n",
      "\n",
      "$$\\boldsymbol{d}\\boldsymbol{a}(x)=f(g(x)),$$\n",
      "\n",
      "which is the spike rate encoded version of $g(x)$ and can be sent as input to another pool via its encoding matrix. That is, the next pool would receive \n",
      "\n",
      "\\begin{align}\n",
      "f_e(x) &= \\max(\\boldsymbol{e}\\boldsymbol{d}\\boldsymbol{a}(x), 0) \\\\\n",
      "f_i(x) &= \\max(-\\boldsymbol{e}\\boldsymbol{d}\\boldsymbol{a}(x), 0)\n",
      "\\end{align}\n",
      "\n",
      "where $\\boldsymbol{e}$ is the next pool's encoding matrix and $\\boldsymbol{a}(x)$ is the current pool's activity vector.\n",
      "\n",
      "If we're not decoding to obtain something that will be an input to another pool but rather to send as the output of the network, we can omit encoding $g(x)$ and find $\\boldsymbol{d}$ such that \n",
      "\n",
      "$$\\boldsymbol{d}\\boldsymbol{a}(x)=g(x).$$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "linear encoding: linear transformations"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Besides sending $g(x)$ to another pool, we sometimes wish to send a linearly transformed version of $g(x)$, $Mg(x)$ where $M$ is an $e\\times d$ transformation matrix. This happens often when a pool fans out to multiple other pools, each of which receives a linearly transformed version of the same $g(x)$. Because the encoding scheme is linear, we can simply apply the transformation matrix to the decoded quantity.\n",
      "\n",
      "\\begin{align}\n",
      "Mf(g(x)) &= Mf_{max}g(x) \\\\\n",
      " &= f_{max}Mg(x) \\\\\n",
      " &= f(Mg(x))\n",
      "\\end{align}\n",
      "\n",
      "That is, we can send\n",
      "\n",
      "$$M\\boldsymbol{d}\\boldsymbol{a}(x)$$\n",
      "\n",
      "as input to the encoding matrix of the target pool. The target pool will then recieve\n",
      "\n",
      "$$\\boldsymbol{e}M\\boldsymbol{d}\\boldsymbol{a}(x)$$\n",
      "\n",
      "or more specifically,\n",
      "\n",
      "\\begin{align}\n",
      "f_e(x) = \\max(\\boldsymbol{e}M\\boldsymbol{d}\\boldsymbol{a}(x), 0) \\\\\n",
      "f_i(x) = \\max(-\\boldsymbol{e}M\\boldsymbol{d}\\boldsymbol{a}(x), 0)\n",
      "\\end{align}\n",
      "\n",
      "as input. Because $e<N$ and $d<N$ always, it is cheaper in terms of memory requirements to store one decode matrix and multiple transformation matrices than to store a unique decode matrix for each linearly transformed version of the $g(x)$. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "linear encoding: input scaling"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Recall the implicit assumption that $-1 \\le x \\le 1$ in the definition with respect to $f_{max}$. When a pool's input comes from an external quantity in the real world, $x$ will range outside of $[-1,1]$. To handle these cases, we define a radius scaling, $r=\\max(|x|)$ to scale the range of $x$ back to within $[-1,1]$. That is we normalize $x$ by $r$ to obtain\n",
      "\n",
      "$$x'=\\frac{x}{r}$$\n",
      "\n",
      "and use $x'$ as our network's input. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "linear encoding: multiple inputs"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The range of $x$ can also exceed $[-1,1]$ when a pool receives multiple inputs at its synapses, which sum the inputs. Say a pool recieved input from $n$ pools with radii, $r_0, r_1, \\ldots r_n$. Because inputs are combined by summation, the pool would effectively receive input of radius $\\sum_{i=1}^nr_i$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There is a range of options we could choose from in this case. At one end, we could normalize all of the inputs by the $\\sum_{i=1}^nr_i$ and send \n",
      "$$\n",
      "\\frac{f(g_1(x)) + f(g_2(x)) + \\ldots f(g_n(x))}{\\sum_{i=1}^nr_i}\n",
      "$$\n",
      "as input to ensure that the input remains within $[-1,1]$.\n",
      "\n",
      "At the other end, we could do no normalization at all and use\n",
      "$$\n",
      "f(g_1(x)) + f(g_2(x)) + \\ldots f(g_n(x)).\n",
      "$$\n",
      "as input."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The tradeoff is between dynamic range and signal-to-noise ratio. Without normalization, a pool could receive spike rates greater than $f_{max}$, which could be oustide the range the pool has been trained to operate in. Whether its decoding matrix still computes the target function given the pool's response depends on whether you've trained the pool with firing rate inputs greater than $f_{max}$. Pools are also liable to saturate in their response as firing rates get very high. However, with normalization, we scale down the input firing rates and so sacrafice the SNR of each input. The SNR for a synapse filtered Poisson spike train $\\propto \\sqrt{\\text{input firing rate}}$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For example, if a pool receives inputs that are largely nonzero and vary widely across their respective radii, then normalization or training outside of $f_{max}$ is probably the way to go. However, if a pool receives multiple inputs but most of them are zero or we know that their sum remains within $[-1,1]$, then leaving the pool unnormalized makes the most sense to preserve the input signals' SNRs."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "affine encoding"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In affine encoding, a variable $x$ is encoded as a spike rate $f(x)$ by scaling and shifting $x$:\n",
      "\n",
      "$$f(x) = xf_{max} + f_{offset}$$\n",
      "and\n",
      "$$x = \\frac{f(x) - f_{offset}}{f_{max}}$$\n",
      "where $f_{max}$ is the maximum firing rate and $f_{offset}$ is the firing rate offset. The excitatory and inhibitory inputs to a pool will then be\n",
      "\n",
      "\\begin{align}\n",
      "f_e(x) &= \\max(\\boldsymbol{e}f(x),0) \\\\\n",
      " &= \\max(\\boldsymbol{e}xf_{max} + \\boldsymbol{e}f_{offset},0) \\\\\n",
      "f_i(x) &= \\max(-\\boldsymbol{e}f(x),0) \\\\\n",
      " &= \\max(-\\boldsymbol{e}xf_{max} - \\boldsymbol{e}f_{offset},0)\n",
      "\\end{align}\n",
      "\n",
      "This is fairly similar to the linear case."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "affine encoding: training"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We find $\\boldsymbol{d}$ such that\n",
      "\n",
      "\\begin{align}\n",
      "\\boldsymbol{d}\\boldsymbol{a}(x) &= f(g(x)) \\\\ \n",
      " &= g(x)f_{max} + f_{offset}\n",
      "\\end{align}\n",
      "\n",
      "just like in the linear encoding case. Also as in the linear encoding case, if we are not sending to another pool, we can omit encoding $g(x)$ and find $\\boldsymbol{d}$ such that \n",
      "\n",
      "$$\\boldsymbol{d}\\boldsymbol{a}(x)=g(x).$$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "affine encoding: linear transformations"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Connections with transformations are complicated by the fact that\n",
      "\n",
      "$$\n",
      "Mf(g(x)) \\neq f(Mg(x))\n",
      "$$\n",
      "\n",
      "That is, \n",
      "\n",
      "\\begin{align}\n",
      "Mf(g(x)) &= M\\left(g(x)f_{max} + f_{offset}\\right) \\\\\n",
      " &= Mg(x)f_{max} + Mf_{offset}\n",
      "\\end{align}\n",
      "whereas\n",
      "$$\n",
      "f(Mg(x)) = Mg(x)f_{max} + f_{offset}\n",
      "$$\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This means that we cannot train the decode matrices for the function $g(x)$ first and then apply the transformation in the connection. To implement the correct encoding of the transformed variable,\n",
      "\n",
      "* We could train $\\boldsymbol{d}$ so that \n",
      "\\begin{align}\n",
      "\\boldsymbol{d}\\boldsymbol{a}(x) &= Mg(x)f_{max} + f_{offset} \\\\\n",
      " &= f(Mg(x))\n",
      "\\end{align}\n",
      "and fold the transformation matrix into the decode matrix.\n",
      "\n",
      "* We could train $\\boldsymbol{d}$ so that \n",
      "$$\n",
      "\\boldsymbol{d}\\boldsymbol{a}(x) = g(x)f_{max} + M^{-1}f_{offset}\n",
      "$$\n",
      "and fold the transformation matrix inverse into the decode matrix so\n",
      "\\begin{align}\n",
      "M\\boldsymbol{d}\\boldsymbol{a}(x) &= Mg(x)f_{max} + f_{offset} \\\\\n",
      " &= f(Mg(x))\n",
      "\\end{align}\n",
      "\n",
      "In both cases, we would have to train the decode matrix to account for a specific $M$ in order to correctly encode $Mg(x)$ as input to the target pool. This is a big problem because we would not be able to use the same decoding matrix with different transformations matrices across different connections. We want to share the decode matrix because its is bigger than a transformation matrix in all practical cases. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "affine encoding: input scaling"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As with linear transformations, we can simply renormalize an external input that exceeds $[-1,1]$. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "affine encoding: multiple inputs"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As the transformation matrices caused problems for affine encoding, so does the case of multiple inputs. Let's say a pool has $n$ inputs $f(g_1(x)), f(g_2(x)), \\ldots, f(g_n(x))$. In the linear case, the encoded inputs simply summed together to represent the encoding of the summed inputs. Here, however,  \n",
      "$$\n",
      "f\\left(\\sum_{i=1}^ng_i(x)\\right) \\neq \\sum_{i=1}^nf(g_i(x)).\n",
      "$$\n",
      "\n",
      "More specifically, \n",
      "$$\n",
      "f\\left(\\sum_{i=1}^ng_i(x)\\right) + (n-1)f_{offset} = \\sum_{i=1}^nf(g_i(x)).\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To get around this, we could adopt the same strategy as in the transformation matrix case by modifying the decoding matrix that produced the individual inputs. That is, for input $i$, we would train $\\boldsymbol{d}_i$ such that\n",
      "$$\n",
      "\\boldsymbol{d}_i\\boldsymbol{a}_i(x)=g_i(x)f_{max}+\\frac{f_{offset}}{n}\n",
      "$$\n",
      "so that\n",
      "\\begin{align}\n",
      "\\sum_{i=1}^{n}\\boldsymbol{d}_i\\boldsymbol{a}_i(x) &= \\sum_{i=1}^{n}\\left(g_i(x)f_{max}+\\frac{f_{offset}}{n}\\right) \\\\\n",
      " &= f\\left(\\sum_{i=1}^ng_i(x)\\right)\n",
      "\\end{align}\n",
      "\n",
      "Again, the problem is that this solution introduces dependencies between the decode matrices and their targets which prevent the same decode matrix from being used across multiple, different connections. Another approach would be to decode the offset separately from one pool or a number of pools and add it back in at each layer of the network, but I really don't like the special cases that this encoding scheme introduces."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Similarly, we would have to handle radius scaling the same way by training individual decode matrices for each fanout connection. Clearly, this approach does not scale well to complicated networks."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "fpfm encoding"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In fpfm encoding, a variable $x$ is encoded in the difference between two spike rates, $f_p$ and $f_m$:\n",
      "\n",
      "\\begin{align}\n",
      "f_p(x) &= \\frac{1+x}{2}f_{max} + f_{offset} \\\\\n",
      "f_m(x) &= \\frac{1-x}{2}f_{max} + f_{offset} \\\\\n",
      "\\end{align}\n",
      "\n",
      "and \n",
      "\n",
      "$$\n",
      "x = \\frac{f_p(x)-f_m(x)}{f_{max}}\n",
      "$$\n",
      "\n",
      "As in the previous encoding schemes, implicit in this definition of the maximum firing rate is the assumption that $-1\\le x\\le 1$.  \n",
      "\n",
      "$f_e$ and $f_i$ will then be\n",
      "\n",
      "\\begin{align}\n",
      "f_e(x) &= \\max(\\boldsymbol{e}f_p(x),0) + \\max(-\\boldsymbol{e}f_m(x),0) \\\\\n",
      "f_i(x) &= \\max(-\\boldsymbol{e}f_p(x),0) + \\max(\\boldsymbol{e}f_m(x),0).\n",
      "\\end{align}"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "fpfm encoding: training"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We find $\\boldsymbol{d}_p$ and $\\boldsymbol{d}_m$ such that \n",
      "\n",
      "\\begin{align}\n",
      "\\boldsymbol{d}_p\\boldsymbol{a}(x) &= f_p(g(x)) \\\\\n",
      "\\boldsymbol{d}_m\\boldsymbol{a}(x) &= f_m(g(x)) \\\\\n",
      "\\end{align}\n",
      "\n",
      "As before, if we are not sending to another pool, we can omit encoding $g(x)$ and just find $\\boldsymbol{d}$ such that \n",
      "\n",
      "$$\\boldsymbol{d}\\boldsymbol{a}(x)=g(x).$$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "fpfm encoding: linear transformations"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Connections with transformations are complicated by the fact that\n",
      "$$\n",
      "Mf_p(g(x)) \\neq f_p(Mg(x))\n",
      "$$\n",
      "and\n",
      "$$\n",
      "Mf_m(g(x)) \\neq f_m(Mg(x))\n",
      "$$\n",
      "\n",
      "That is, \n",
      "\n",
      "$$\n",
      "Mf_p(g(x)) = M\\left(\\frac{1+g(x)}{2}f_{max} + f_{offset}\\right)\n",
      "$$\n",
      "whereas\n",
      "$$\n",
      "f_p(Mg(x)) = \\frac{1+Mg(x)}{2}f_{max} + f_{offset},\n",
      "$$\n",
      "\n",
      "and \n",
      "\n",
      "$$\n",
      "Mf_m(g(x)) = M\\left(\\frac{1-g(x)}{2}f_{max} + f_{offset}\\right)\n",
      "$$\n",
      "whereas\n",
      "$$\n",
      "f_m(Mg(x)) = \\frac{1-Mg(x)}{2}f_{max} + f_{offset}.\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This means that we cannot train the decode matrices for the function $g(x)$ first and then apply the transformation in the connection but would have to train the decode matrices for $Mg(x)$ in order to correctly encode $Mg(x)$ as input to the target pool."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "However, if we cross-couple $f_p(x)$ and $f_m(x)$ in the transformation, we can acheive the desired encoding of the transformed input to the target pool. That is, we can find $M_{pp}$, $M_{pm}$, $M_{mp}$, and $M_{mm}$ such that\n",
      "\n",
      "\\begin{align}\n",
      "f_p(Mg(x)) &= M_{pp}f_p(g(x)) + M_{pm}f_m(g(x)) \\\\\n",
      "f_m(Mg(x)) &= M_{mp}f_p(g(x)) + M_{mm}f_m(g(x))\n",
      "\\end{align}\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Consider the $f_p$ case first.\n",
      "\n",
      "\\begin{align}\n",
      "f_p(Mg(x))&=\\frac{1+Mg(x)}{2}f_{max} + f_{offset} \\\\\n",
      " &= \\frac{f_{max}}{2}+\\frac{f_{max}}{2}Mg(x) + f_{offset}\n",
      "\\end{align}\n",
      "\n",
      "For $M_{pp}$ and $M_{pm}$, we have\n",
      "\n",
      "\\begin{align}\n",
      "M_{pp}f_p(g(x)) + M_{pm}f_m(g(x)) &= \n",
      "  M_{pp}\\left(\\frac{1+g(x)}{2}f_{max} + f_{offset}\\right) + M_{pm}\\left(\\frac{1-g(x)}{2}f_{max} + f_{offset}\\right) \\\\\n",
      " &= \\frac{f_{max}}{2}M_{pp} + \\frac{f_{max}}{2}M_{pp}g(x) + f_{offset}M_{pp} +\n",
      "    \\frac{f_{max}}{2}M_{pm} - \\frac{f_{max}}{2}M_{pm}g(x) + f_{offset}M_{pm} \\\\\n",
      " &= \\frac{f_{max}}{2}(M_{pp}+M_{pm})+\\frac{f_{max}}{2}(M_{pp}-M_{pm})g(x)+f_{offset}(M_{pp}+M_{pm})\n",
      "\\end{align}\n",
      "\n",
      "Comparing this to $f_p(Mg(x))$, we see that \n",
      "\n",
      "\\begin{align}\n",
      "M_{pp}+M_{pm} &= I \\\\\n",
      "M_{pp}-M_{pm} &= M\n",
      "\\end{align}\n",
      "\n",
      "so,\n",
      "\n",
      "\\begin{align}\n",
      "M_{pp} &= \\frac{I+M}{2} \\\\\n",
      "M_{pm} &= \\frac{I-M}{2}\n",
      "\\end{align}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now consider the $f_m$ case.\n",
      "\n",
      "\\begin{align}\n",
      "f_m(Mg(x))&=\\frac{1-Mg(x)}{2}f_{max} + f_{offset} \\\\\n",
      " &= \\frac{f_{max}}{2}-\\frac{f_{max}g(x)}{2}M + f_{offset}\n",
      "\\end{align}\n",
      "\n",
      "For $M_{mp}$ and $M_{mm}$, we have\n",
      "\n",
      "\\begin{align}\n",
      "M_{mp}f_p(g(x)) + M_{mm}f_m(g(x)) &= \n",
      "  M_{mp}\\left(\\frac{1+g(x)}{2}f_{max} + f_{offset}\\right) + M_{mm}\\left(\\frac{1-g(x)}{2}f_{max} + f_{offset}\\right) \\\\\n",
      " &= \\frac{f_{max}}{2}M_{mp} + \\frac{f_{max}g(x)}{2}M_{mp} + f_{offset}M_{mp} +\n",
      "    \\frac{f_{max}}{2}M_{mm} - \\frac{f_{max}g(x)}{2}M_{mm} + f_{offset}M_{mm} \\\\\n",
      " &= \\frac{f_{max}}{2}(M_{mp}+M_{mm})+\\frac{f_{max}}{2}(M_{mp}-M_{mm})g(x)+f_{offset}(M_{mp}+M_{mm})\n",
      "\\end{align}\n",
      "\n",
      "Comparing this to $f_m(Mg(x))$, we see that \n",
      "\n",
      "\\begin{align}\n",
      "M_{mp}+M_{mm} &= I \\\\\n",
      "M_{mp}-M_{mm} &= -M\n",
      "\\end{align}\n",
      "\n",
      "so,\n",
      "\\begin{align}\n",
      "M_{mp} &= \\frac{I-M}{2} \\\\\n",
      "M_{mm} &= \\frac{I+M}{2}\n",
      "\\end{align}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**To summarize:**\n",
      "\n",
      "\\begin{align}\n",
      "M_{pp} &= \\frac{I+M}{2} \\\\\n",
      "M_{pm} &= \\frac{I-M}{2} \\\\\n",
      "M_{mp} &= \\frac{I-M}{2} \\\\\n",
      "M_{mm} &= \\frac{I+M}{2}\n",
      "\\end{align}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Putting it all together, we see that we can encode $Mg(x)$ and share decoders by cross coupling $f_p(g(x))$ and $f_m(g(x))$. That is, to deliver $f_p(Mg(x))$ and $(f_m(Mg(x))$, we deliver $f_p'(g(x))$ and $f_m'(g(x))$ where\n",
      "\n",
      "\\begin{align}\n",
      "f_p'(g(x)) &= M_{pp}f_p(g(x)) + M_{pm}f_m(g(x)) \\\\\n",
      " &= \\frac{I+M}{2}f_p(g(x)) + \\frac{I-M}{2}f_m(g(x)) \\\\\n",
      " &= f_p(Mg(x))\n",
      "\\end{align}\n",
      "\n",
      "and\n",
      "\n",
      "\\begin{align}\n",
      "f_m'(g(x)) &= M_{mp}f_p(g(x)) + M_{mm}f_m(g(x)) \\\\\n",
      " &= \\frac{I-M}{2}f_p(g(x)) + \\frac{I+M}{2}f_m(g(x)) \\\\\n",
      " &= f_m(Mg(x))\n",
      "\\end{align}\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "fpfm encoding: linear transformations: generalizing to non-square $M$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If $M$ isn't square, then the above formulae for $M_{pp}$, $M_{pm}$, $M_{mp}$, and $M_{mm}$ will fail because $I$ is square by definition. However, we can adjust our formulae to accomodate non-square $M$. For an $e\\times d$ dimensional $M$, let's start by making the vector sizes explicit in our encoding scheme:\n",
      "\n",
      "\\begin{align}\n",
      "f_p(g(x)) &= \\frac{f_{max}}{2}\\boldsymbol{1}_d + \\frac{f_{max}}{2}g(x) + f_{offset}\\boldsymbol{1}_d \\\\\n",
      "f_m(g(x)) &= \\frac{f_{max}}{2}\\boldsymbol{1}_d - \\frac{f_{max}}{2}g(x) + f_{offset}\\boldsymbol{1}_d \\\\\n",
      "\\end{align}\n",
      "\n",
      "and\n",
      "\n",
      "\\begin{align}\n",
      "f_p(Mg(x)) &= \\frac{f_{max}}{2}\\boldsymbol{1}_e + \\frac{f_{max}}{2}Mg(x) + f_{offset}\\boldsymbol{1}_e \\\\\n",
      "f_m(Mg(x)) &= \\frac{f_{max}}{2}\\boldsymbol{1}_e - \\frac{f_{max}}{2}Mg(x) + f_{offset}\\boldsymbol{1}_e \\\\\n",
      "\\end{align}\n",
      "\n",
      "where $\\boldsymbol{1}_d$ and $\\boldsymbol{1}_e$ are $1$-dimensional vectors of length $d$ or $e$, respectively. Note the change in dimensionality caused by the introduction of $M$. \n",
      "\n",
      "Recall the equations we solved for $M_{pp}$, $M_{pm}$, $M_{mp}$, and $M_{mm}$ with explicit dimensions:\n",
      "\n",
      "\\begin{align}\n",
      "M_{pp}f_p(g(x)) + M_{pm}f_m(g(x)) &= \\ldots \\\\\n",
      " &= \\frac{f_{max}}{2}(M_{pp}+M_{pm})\\boldsymbol{1}_d+\n",
      "  \\frac{f_{max}}{2}(M_{pp}-M_{pm})g(x)+\n",
      "  f_{offset}(M_{pp}+M_{pm})\\boldsymbol{1}_d \\\\\n",
      "M_{mp}f_p(g(x)) + M_{mm}f_m(g(x)) &= \\ldots \\\\\n",
      " &= \\frac{f_{max}}{2}(M_{mp}+M_{mm})\\boldsymbol{1}_d+\n",
      "  \\frac{f_{max}}{2}(M_{mp}-M_{mm})g(x)+\n",
      "  f_{offset}(M_{mp}+M_{mm})\\boldsymbol{1}_d\n",
      "\\end{align}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Comparing these with our equations with $f_p(Mg(x))$ and $f_m(Mg(x))$, we see that\n",
      "\n",
      "\\begin{align}\n",
      "(M_{pp}+M_{pm})\\boldsymbol{1}_d &= \\boldsymbol{1}_e \\\\\n",
      "M_{pp}-M_{pm} &= M \\\\\n",
      "(M_{mp}+M_{mm})\\boldsymbol{1}_d &= \\boldsymbol{1}_e \\\\\n",
      "M_{mp}-M_{mm} &= -M. \\\\\n",
      "\\end{align}\n",
      "\n",
      "The equations with $M$ are unchanged. The equations relating $\\boldsymbol{1}_d$ to $\\boldsymbol{1}_e$ only imply that the rows of $(M_{pp}+M_{pm})$ and $(M_{mp} + M_{mm})$ sum to 1. Therefore, we can write \n",
      "\n",
      "\\begin{align}\n",
      "M_{pp}+M_{pm} &= \\frac{1}{d}\\boldsymbol{1}_{e\\times d} \\\\\n",
      "M_{pp}-M_{pm} &= M,\n",
      "\\end{align}\n",
      "\n",
      "and \n",
      "\n",
      "\\begin{align}\n",
      "M_{mp}+M_{mm} &= \\frac{1}{d}\\boldsymbol{1}_{e\\times d} \\\\\n",
      "M_{mp}-M_{mm} &= -M.\n",
      "\\end{align}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "so\n",
      "\\begin{align}\n",
      "M_{pp} &= \\frac{\\frac{1}{d}\\boldsymbol{1}_{e\\times d} + M}{2}\\\\\n",
      "M_{pm} &= \\frac{\\frac{1}{d}\\boldsymbol{1}_{e\\times d} - M}{2}\\\\\n",
      "M_{mp} &= \\frac{\\frac{1}{d}\\boldsymbol{1}_{e\\times d} - M}{2}\\\\\n",
      "M_{mm} &= \\frac{\\frac{1}{d}\\boldsymbol{1}_{e\\times d} + M}{2}\\\\\n",
      "\\end{align}"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "fpfm encoding: input scaling"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here again, we're going to have to train different decode matrices for different connections if they require normalization."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}